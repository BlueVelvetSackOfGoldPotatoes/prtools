<html><head><title>Hidden Markov Model (HMM) Toolbox for Matlab</title></head><body bgcolor="#ffffff">


<!-- white background is better for the pictures and equations -->



<h1>Hidden Markov Model (HMM) Toolbox for Matlab</h1>
Written by Kevin Murphy, 1998.<br>
Last updated: 7 June 2004.
<br>
<b>
The interface has changed!
This documentation refers to the version of 4 May 2003 or later.
</b>

<p>
This toolbox supports inference and learning for
HMMs with discrete outputs (dhmm's), Gaussian outputs (ghmm's),
or mixtures of Gaussians output (mhmm's).
The Gaussians can be full, diagonal, or spherical (isotropic).
It also supports discrete inputs, as in a POMDP.
The inference routines support filtering, smoothing, and fixed-lag smoothing.
For more general models, please see my
<a href="http://www.ai.mit.edu/%7Emurphyk/Software/BNT/bnt.html">Bayes Net Toolbox</a>.

</p><ul>
<li> <a href="#hmm">What is an HMM?</a>
</li><li> <a href="#usage">How to use the HMM toolbox</a>
</li><li> <a href="#other">Other matlab software for HMMs</a>
</li><li> <a href="#read">Recommended reading</a>
</li><li> <a href="http://www.ai.mit.edu/%7Emurphyk/Software/HMM/hmm_download.html">Download toolbox</a>
</li></ul>

<p>

</p><p>
</p><h1 label="hmm">What is an HMM?</h1>

An HMM is a Markov chain, where each state generates an
observation. You only see the observations, and the goal is to infer
the hidden state sequence.
For example, the hidden states may represent words or phonemes,
and the observations represent the acoustic signal.
Please see <a href="#read">recommended reading</a> for details.


<h1><a name="usage">How to use the HMM toolbox</a></h1>

<h2><a name="usage">HMMs with discrete outputs</a></h2>

<h3><a name="usage">Maximum likelihood parameter estimation using EM (Baum Welch)</a></h3>

<a name="usage">The script dhmm_em_demo.m gives an example of how to learn an HMM
with discrete outputs. Let there be Q=2 states and O=2 output
symbols. We create random stochastic matrices as follows.
</a><pre><a name="usage">O = 3;
Q = 2;
prior0 = normalise(rand(Q,1));
transmat0 = mk_stochastic(rand(Q,Q));
obsmat0 = mk_stochastic(rand(Q,O));  
</a></pre>
<a name="usage">Now we sample nex=20 sequences of length T=10 each from this model, to
use as training data.
</a><pre><a name="usage">T=10;
nex=20;
data = dhmm_sample(prior0, transmat0, obsmat0, nex, T);  
</a></pre>
<a name="usage">Now we make a random guess as to what the parameters are,
</a><pre><a name="usage">prior1 = normalise(rand(Q,1));
transmat1 = mk_stochastic(rand(Q,Q));
obsmat1 = mk_stochastic(rand(Q,O));
</a></pre>
<a name="usage">and  improve our guess using 5 iterations of EM...
</a><pre><a name="usage">[LL, prior2, transmat2, obsmat2] = dhmm_em(data, prior1, transmat1, obsmat1, 'max_iter', 5);
</a></pre>
<a name="usage">LL(t) is the log-likelihood after iteration t, so we can plot the
learning curve.


</a><h3><a name="usage">Sequence classification</a></h3>

<a name="usage">To evaluate the log-likelihood of a trained model given test data,
proceed as follows:
</a><pre><a name="usage">loglik = dhmm_logprob(data, prior, transmat, obsmat)
</a></pre>
<a name="usage">Note: the discrete alphabet is assumed to be {1, 2, ..., O},
where O = size(obsmat, 2). Hence data cannot contain any 0s.
</a><p>
<a name="usage">To classify a sequence into one of k classes, train up k HMMs, one per
class, and then compute the log-likelihood that each model gives to
the test sequence; if the i'th model is the most likely, then declare
the class of the sequence to be class i.

</a></p><h3><a name="usage">Computing the most probable sequence (Viterbi)</a></h3>

<a name="usage">First you need to evaluate B(i,t) = P(y_t | Q_t=i) for all t,i:
</a><pre><a name="usage">B = multinomial_prob(data, obsmat);
</a></pre>
<a name="usage">Then you can use
</a><pre><a name="usage">[path, loglik] = viterbi_path(prior, transmat, B)   
</a></pre>


<h2><a name="usage">HMMs with mixture of Gaussians outputs</a></h2>

<h3><a name="usage">Maximum likelihood parameter estimation using EM (Baum Welch)</a></h3>

<a name="usage">Let us generate nex=50 vector-valued sequences of length T=50; each
vector has size O=2.
</a><pre><a name="usage">O = 2;
T = 50;
nex = 50;
data = randn(O,T,nex);
</a></pre>
<a name="usage">Now let use fit a mixture of M=2 Gaussians for each of the Q=2 states
using K-means.
</a><pre><a name="usage">M = 2;
Q = 2;
left_right = 0;

prior0 = normalise(rand(Q,1));
transmat0 = mk_stochastic(rand(Q,Q));

[mu0, Sigma0] = mixgauss_init(Q*M,data, cov_type);
mu0 = reshape(mu0, [O Q M]);
Sigma0 = reshape(Sigma0, [O O Q M]);
mixmat0 = mk_stochastic(rand(Q,M));

</a></pre>
<a name="usage">Finally, let us improve these parameter estimates using EM.
</a><pre><a name="usage">[LL, prior1, transmat1, mu1, Sigma1, mixmat1] = ...
    mhmm_em(data, prior0, transmat0, mu0, Sigma0, mixmat0, 'max_iter', 2);
</a></pre>
<a name="usage">Since EM only finds a local optimum, good initialisation is crucial.
<b>The initialisation procedure illustrated above is very crude, and is probably
not adequate for real applications...</b>
Click </a><a href="http://www.media.mit.edu/wearables/mithril/BNT/mixtureBNT.txt">here</a>
for a real-world example of EM with mixtures of Gaussians using BNT.
<p>


</p><h3><a name="loglikpos">What to do if the log-likelihood becomes positive?</a></h3>

<a name="loglikpos"><b>It is possible for p(x) &gt; 1 if p(x) is a probability density
function</b>, such as a Gaussian. (The requirements for a density are
p(x)&gt;0 for all x and int_x p(x) = 1.) In practice this usually means your
covariance is shrinking to a point/delta function, so you should
increase the width of the prior
(see </a><a href="#priorcov">below</a>),
or constrain the matrix to be spherical 
or diagonal, or clamp it to a large fixed constant (not learn it at all).
It is also very helpful to ensure the
components of the data vectors  have small and comparable magnitudes
(use e.g., KPMstats/standardize).

<p>
This is a well-known pathology of maximum likelihood estimation for
Gaussian mixtures: the global optimum may place one mixture component
on a single data point, and give it 0 covariance, and hence infinite
likelihood. One usually relies on the fact that EM cannot find the
global optimum to avoid such pathologies.

</p><h3><a name="decreasingloglik">What to do if the log-likelihood
decreases during EM?</a></h3>

<a name="decreasingloglik">Since I implicitly add a prior to every covariance matrix
(see </a><a href="#priorcov">below</a>), what increases is loglik + log(prior),
but what I print is just loglik, which may occasionally decrease.
This suggests that one of your mixture components is not getting
enough data. Try a better initialization or fewer clusters (states).

<h3><a name="singularcov">What to do if the covariance matrix becomes singular?</a></h3>

<a name="singularcov">Estimates of the covariance matrix often become singular if you have
too little data, or if too few points are assigned to a cluster center
due to a bad initialization of the means.
In this case, you should constrain the covariance to
be spherical or diagonal, or adjust the prior (see </a><a href="#priorcov">below</a>), or try a
better initialization.

<h3><a name="priorcov">How do I add a prior to the covariance matrix?</a></h3>

<a name="priorcov">Buried inside of KPMstats/mixgauss_Mstep you will see that cov_prior
is initialized to 0.01*I. This is added to the maximum likelihood
estimate after every M step.
To change this, you will need to modify the
mhmm_em function so it calls mixgauss_Mstep with a different value.
 



</a><h3><a name="priorcov">Sequence classification</a></h3>

<a name="priorcov">To classify a sequence (e.g., of speech) into one of k classes (e.g.,
the digits 0-9), proceed as in the DHMM case above,
but use the following procedure to compute likelihood:
</a><pre><a name="priorcov">loglik = mhmm_logprob(data, prior, transmat, mu, Sigma, mixmat);
</a></pre>


<h3><a name="priorcov">Computing the most probable sequence (Viterbi)</a></h3>

<a name="priorcov">First you need to evaluate B(t,i) = P(y_t | Q_t=i) for all t,i:
</a><pre><a name="priorcov">B = mixgauss_prob(data, mu, Sigma, mixmat);
</a></pre>
<a name="priorcov">Finally, use
</a><pre><a name="priorcov">[path, loglik] = viterbi_path(prior, transmat, B);
</a></pre>


<h2><a name="priorcov">HMMs with Gaussian outputs</a></h2>

<a name="priorcov">This is just like the mixture of Gaussians case,
except we have M=1, and hence there is no mixing matrix.
<!--
The learning routine is called as follows:
<pre>
[LL, prior1, transmat1, mu1, Sigma1] = ...
    mhmm_em(data, prior0, transmat0, mu0, Sigma0,  max_iter);
</pre>
The classification routine is called as follows:
<pre>
loglik = log_lik_ghmm(data, prior, transmat, mu, Sigma);
</pre>
The likelihood routine is called as
<pre>
B = eval_pdf_cond_gauss(data, mu, Sigma);
</pre>
-->

</a><h2><a name="priorcov">Online EM for discrete HMMs/ POMDPs</a></h2>

<a name="priorcov">For some applications (e.g., reinforcement learning/ adaptive
control), it is necessary to learn a model online.
The script dhmm_em_online_demo gives an example of how to do this.




</a><h1><a name="other">Other packages for HMMs</a></h1>

<ul>

<li><a name="other"> </a><a href="http://htk.eng.cam.ac.uk/">HTK3</a> from Cambridge
University is open source C code for HMMs for speech recognition.

</li><li><a href="http://www.mathworks.com/access/helpdesk/help/toolbox/stats/hidden_m.shtml">Mathworks
stats toolbox 4.1</a> contains some functions for discrete HMMs.

</li><li> 
<a href="http://www.gatsby.ucl.ac.uk/%7Ezoubin/software.html">Zoubin Ghahramani</a>
has matlab code which is very similar to mine (but doesn't handle mhmm's).
He also has code for approximate (variational) inference in factorial HMMs.

<!--
<li>
<a href="http://bunny.la.asu.edu/jialongsweb/software.htm">
MEX and C++ files</a> for training DHMMs and GHMMs, etc.
-->


</li><li><a href="http://www.ee.ic.ac.uk/hp/staff/dmb/voicebox/voicebox.html">
Matlab Speech processing toolbox</a>

</li><li>
<a href="http://www.eeng.dcu.ie/%7Espeech5/matspeech.html">
More matlab speech processing routines</a>

</li><li>
<a href="http://www.zaik.uni-koeln.de/%7Ehmm/ghmm/">GNU HMM toolbox</a>
implements the basic algorithms.

</li></ul>




<h1><a name="read">Recommended reading on HMMs</a></h1>
<ul>

<li><a name="read"> </a><a href="http://www.ai.mit.edu/%7Emurphyk/Software/HMM/rabiner.pdf">A tutorial on Hidden Markov Models and
selected applications in speech recognition</a>,
L. Rabiner, 1989, Proc. IEEE 77(2):257--286.

</li><li> <a href="http://www.ee.washington.edu/techsite/papers/documents/UWEETR-2002-0003.pdf">What
 HMMs can do</a>, Jeff Bilmes, U. Washington Tech Report, Feb 2002

</li><li> <a href="http://www.icsi.berkeley.edu/%7Ejagota/NCS/vol2.html">
Markovian Models for Sequential Data</a>,
Y. Bengio, Neural Computing Surveys 2, 129--162, 1999.

</li><li>
<a href="http://www.datalab.uci.edu/papers/hmmpin.pdf">
Probabilistic independence networks for hidden Markov models</a>
P. Smyth, D. Heckerman, and M. Jordan,
Neural Computation , vol.9, no. 2, 227--269, 1997. 

<!--
<li> "Statistical Methods for Speech Recognition", F. Jelinek, MIT Press 1997.
-->

</li><li> Notes from <a href="http://www.ee.columbia.edu/%7Edpwe/e6820/outline.html">
Dan Ellis'</a> class on speech processing at Columbia (2002).
I snagged the excellent notes for
<a href="http://www.ai.mit.edu/%7Emurphyk/Software/HMM/E6820-L10-ASR-seq.pdf">lecture 10</a>
and
<a href="http://www.ee.columbia.edu/%7Edpwe/e6820/lectures/E6820-L11-ASR-sys.pdf">
Lecture 11</a>.

</li><li> Bourlard's
<!--
<a href="http://www.ee.columbia.edu/~dpwe/e6820/ matlab/epflhmm/labman2.pdf">
-->
<a href="http://www.ai.mit.edu/%7Emurphyk/Software/HMM/labman2.pdf">
EPFL matlab lab manual on HMMs</a>.


<!--
<li>  <a
href="http://www.gatsby.ucl.ac.uk/~zoubin/papers/fhmmML.ps.gz">Factorial
Hidden Markov Models</a>,
 Z. Ghahramani and M. Jordan,   Machine Learning 29:245--273, 1997.
-->


</li><li> <a href="http://www-sig.enst.fr/%7Ecappe/docs/hmmbib.html">Bibliography on
HMMs</a> (2001)

</li><li> <a href="http://www.cs.bham.ac.uk/%7Ejlw/bookmarks.html">Bookmarks
on HMMs</a>

</li><li> <a href="http://www.cs.orst.edu/%7Etgd/publications/mlsd-ssspr.pdf">
 Machine Learning for Sequential Data: A
Review. </a>
Tom Dietterich. In T. Caelli (Ed.) Lecture Notes in Computer
Science (2002).

</li></ul>
</body></html>